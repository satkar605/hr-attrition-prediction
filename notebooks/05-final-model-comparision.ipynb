{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69d56c51",
   "metadata": {},
   "source": [
    "# Final Model Comparison: All Models, All Balancing Schemes\n",
    "\n",
    "This notebook compares all machine learning models trained for attrition prediction:\n",
    "- **Logistic Regression**\n",
    "- **Decision Tree**\n",
    "- **Random Forest**\n",
    "- **Gradient Boosting**\n",
    "- **SVC (RBF Kernel)**\n",
    "- **MLP (Neural Network)**\n",
    "\n",
    "Each model is evaluated on three datasets:\n",
    "- **Raw / Original** (Imbalanced)\n",
    "- **SMOTE-Balanced** (Synthetic Oversampling)\n",
    "- **ADASYN-Balanced** (Adaptive Synthetic Oversampling)\n",
    "\n",
    "## Metrics\n",
    "All results are evaluated on the same test set(s) using:\n",
    "- Accuracy\n",
    "- Precision (positive class)\n",
    "- Recall (positive class, focus metric)\n",
    "- F1 (positive class)\n",
    "- ROC AUC\n",
    "- Confusion Matrix (TN, FP, FN, TP)\n",
    "\n",
    "**Objective:**\n",
    "- Aggregate all scoring results, highlight the best overall model/dataset per metric, and deliver business-facing recommendations.\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf146c5a",
   "metadata": {},
   "source": [
    "## Step 1: Aggregation and Loading of All Model Results\n",
    "\n",
    "> ‚ö†Ô∏è **Instruction:** If you have stored metrics as pickles or CSVs from each modeling notebook, load them below. Otherwise, manually copy the `all_metrics` lists / DataFrames (with all relevant fields) from the end of each notebook.\n",
    "\n",
    "**Expected unified DataFrame columns:**\n",
    "- `model_name`, `dataset_type`, `accuracy`, `precision_positive`, `recall_positive`, `f1_positive`, `roc_auc`, `tn`, `fp`, `fn`, `tp`\n",
    "\n",
    "This cell loads and combines results from all notebooks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fce73b5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "EXTRACTING METRICS FROM ALL NOTEBOOKS\n",
      "======================================================================\n",
      "\n",
      "Extracting from notebook 03 (Tree-based models)...\n",
      "  Found 0 entries from notebook 03\n",
      "\n",
      "Extracting from notebook 04 (SVC and MLP)...\n",
      "  Found 0 entries from notebook 04\n",
      "\n",
      "======================================================================\n",
      "NOTEBOOK 02 (Logistic Regression) - Manual Entry Required\n",
      "======================================================================\n",
      "After running notebook 02, extract metrics from model_results['comparison_df']\n",
      "and model_results['raw/smote/adasyn']['confusion_matrix']\n",
      "\n",
      "Template (uncomment and fill after running notebook 02):\n",
      "# metrics_logreg = [\n",
      "#     {'model_name': 'LogisticRegression', 'dataset_type': 'raw', ...},\n",
      "#     {'model_name': 'LogisticRegression', 'dataset_type': 'smote', ...},\n",
      "#     {'model_name': 'LogisticRegression', 'dataset_type': 'adasyn', ...}\n",
      "# ]\n",
      "# all_combined_metrics.extend(metrics_logreg)\n",
      "\n",
      "======================================================================\n",
      "WARNING: No metrics extracted.\n",
      "Please ensure notebooks 03 and 04 have been executed completely.\n",
      "======================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>dataset_type</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision_positive</th>\n",
       "      <th>recall_positive</th>\n",
       "      <th>f1_positive</th>\n",
       "      <th>roc_auc</th>\n",
       "      <th>tn</th>\n",
       "      <th>fp</th>\n",
       "      <th>fn</th>\n",
       "      <th>tp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [model_name, dataset_type, accuracy, precision_positive, recall_positive, f1_positive, roc_auc, tn, fp, fn, tp]\n",
       "Index: []"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Automatically Extract and Combine Metrics from All Notebooks\n",
    "# ============================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "\n",
    "def extract_metrics_from_notebook03():\n",
    "    \"\"\"Extract metrics from notebook 03 (tree-based models).\"\"\"\n",
    "    notebook_path = '../03-tree-based-models.ipynb'\n",
    "    if not os.path.exists(notebook_path):\n",
    "        return []\n",
    "    \n",
    "    try:\n",
    "        with open(notebook_path, 'r') as f:\n",
    "            nb = json.load(f)\n",
    "        \n",
    "        metrics_list = []\n",
    "        cm_data = {}  # Store confusion matrix data by model-dataset\n",
    "        \n",
    "        # First pass: Extract performance metrics\n",
    "        for cell in nb['cells']:\n",
    "            if cell['cell_type'] == 'code' and 'outputs' in cell:\n",
    "                for output in cell['outputs']:\n",
    "                    if 'text' in output:\n",
    "                        text = ''.join(output['text'])\n",
    "                        \n",
    "                        # Extract performance metrics\n",
    "                        if 'PERFORMANCE METRICS' in text or 'COMPREHENSIVE METRICS' in text:\n",
    "                            lines = text.split('\\n')\n",
    "                            for line in lines:\n",
    "                                if any(m in line for m in ['DecisionTree', 'RandomForest', 'GradientBoosting']):\n",
    "                                    # Parse line like: \"    DecisionTree    Raw/Original  0.727891   0.326241 0.647887 0.433962 0.692977\"\n",
    "                                    parts = line.split()\n",
    "                                    if len(parts) >= 7:\n",
    "                                        try:\n",
    "                                            # Find model name (first non-empty word)\n",
    "                                            model_idx = 0\n",
    "                                            while model_idx < len(parts) and not parts[model_idx]:\n",
    "                                                model_idx += 1\n",
    "                                            model = parts[model_idx] if model_idx < len(parts) else None\n",
    "                                            \n",
    "                                            # Find dataset (next 1-2 words)\n",
    "                                            dataset_start = model_idx + 1\n",
    "                                            if dataset_start < len(parts):\n",
    "                                                if dataset_start + 1 < len(parts) and parts[dataset_start + 1] in ['Balanced', 'Original']:\n",
    "                                                    dataset_str = ' '.join(parts[dataset_start:dataset_start+2])\n",
    "                                                    num_start = dataset_start + 2\n",
    "                                                else:\n",
    "                                                    dataset_str = parts[dataset_start]\n",
    "                                                    num_start = dataset_start + 1\n",
    "                                                \n",
    "                                                dataset_type = 'raw' if 'Raw' in dataset_str or 'Original' in dataset_str else ('smote' if 'SMOTE' in dataset_str else 'adasyn')\n",
    "                                                \n",
    "                                                # Extract numeric values\n",
    "                                                nums = []\n",
    "                                                for p in parts[num_start:]:\n",
    "                                                    try:\n",
    "                                                        nums.append(float(p))\n",
    "                                                    except:\n",
    "                                                        pass\n",
    "                                                \n",
    "                                                if len(nums) >= 5 and model:\n",
    "                                                    metrics_list.append({\n",
    "                                                        'model_name': model,\n",
    "                                                        'dataset_type': dataset_type,\n",
    "                                                        'accuracy': nums[0],\n",
    "                                                        'precision_positive': nums[1],\n",
    "                                                        'recall_positive': nums[2],\n",
    "                                                        'f1_positive': nums[3],\n",
    "                                                        'roc_auc': nums[4],\n",
    "                                                        'tn': 0, 'fp': 0, 'fn': 0, 'tp': 0\n",
    "                                                    })\n",
    "                                        except Exception as e:\n",
    "                                            pass\n",
    "                        \n",
    "                        # Extract confusion matrix components\n",
    "                        if 'CONFUSION MATRIX COMPONENTS' in text or 'Confusion Matrix Components' in text:\n",
    "                            lines = text.split('\\n')\n",
    "                            for line in lines:\n",
    "                                if any(m in line for m in ['DecisionTree', 'RandomForest', 'GradientBoosting']):\n",
    "                                    # Parse line like: \"    DecisionTree    Raw/Original 275  95  25  46\"\n",
    "                                    parts = line.split()\n",
    "                                    if len(parts) >= 7:\n",
    "                                        try:\n",
    "                                            # Find model name\n",
    "                                            model_idx = 0\n",
    "                                            while model_idx < len(parts) and not parts[model_idx]:\n",
    "                                                model_idx += 1\n",
    "                                            model = parts[model_idx] if model_idx < len(parts) else None\n",
    "                                            \n",
    "                                            # Find dataset\n",
    "                                            dataset_start = model_idx + 1\n",
    "                                            if dataset_start < len(parts):\n",
    "                                                if dataset_start + 1 < len(parts) and parts[dataset_start + 1] in ['Balanced', 'Original']:\n",
    "                                                    dataset_str = ' '.join(parts[dataset_start:dataset_start+2])\n",
    "                                                    num_start = dataset_start + 2\n",
    "                                                else:\n",
    "                                                    dataset_str = parts[dataset_start]\n",
    "                                                    num_start = dataset_start + 1\n",
    "                                                \n",
    "                                                dataset_type = 'raw' if 'Raw' in dataset_str or 'Original' in dataset_str else ('smote' if 'SMOTE' in dataset_str else 'adasyn')\n",
    "                                                \n",
    "                                                # Get integers for confusion matrix\n",
    "                                                nums = [int(p) for p in parts[num_start:] if p.isdigit()]\n",
    "                                                if len(nums) >= 4 and model:\n",
    "                                                    key = f\"{model}_{dataset_type}\"\n",
    "                                                    cm_data[key] = {'tn': nums[0], 'fp': nums[1], 'fn': nums[2], 'tp': nums[3]}\n",
    "                                        except:\n",
    "                                            pass\n",
    "        \n",
    "        # Merge confusion matrix data into metrics\n",
    "        for metric in metrics_list:\n",
    "            key = f\"{metric['model_name']}_{metric['dataset_type']}\"\n",
    "            if key in cm_data:\n",
    "                metric.update(cm_data[key])\n",
    "        \n",
    "        return metrics_list\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting from notebook 03: {e}\")\n",
    "        return []\n",
    "\n",
    "def extract_metrics_from_notebook04():\n",
    "    \"\"\"Extract metrics from notebook 04 (SVC and MLP).\"\"\"\n",
    "    notebook_path = '../04-svc-nn-models.ipynb'\n",
    "    if not os.path.exists(notebook_path):\n",
    "        return []\n",
    "    \n",
    "    try:\n",
    "        with open(notebook_path, 'r') as f:\n",
    "            nb = json.load(f)\n",
    "        \n",
    "        metrics_list = []\n",
    "        cm_data = {}  # Store confusion matrix data by model-dataset\n",
    "        \n",
    "        # First pass: Extract performance metrics\n",
    "        for cell in nb['cells']:\n",
    "            if cell['cell_type'] == 'code' and 'outputs' in cell:\n",
    "                for output in cell['outputs']:\n",
    "                    if 'text' in output:\n",
    "                        text = ''.join(output['text'])\n",
    "                        \n",
    "                        # Extract performance metrics\n",
    "                        if 'Performance Metrics' in text or 'COMPREHENSIVE METRICS' in text:\n",
    "                            lines = text.split('\\n')\n",
    "                            for line in lines:\n",
    "                                if 'SVC' in line or 'MLP' in line:\n",
    "                                    # Parse line like: \"       SVC    Raw/Original  0.857143            0.560976         0.489362     0.522727 0.778620\"\n",
    "                                    parts = line.split()\n",
    "                                    if len(parts) >= 7:\n",
    "                                        try:\n",
    "                                            # Find model name (first non-empty word that's SVC or MLP)\n",
    "                                            model_idx = 0\n",
    "                                            while model_idx < len(parts) and parts[model_idx] not in ['SVC', 'MLP']:\n",
    "                                                model_idx += 1\n",
    "                                            model = parts[model_idx] if model_idx < len(parts) else None\n",
    "                                            \n",
    "                                            # Find dataset (next 1-2 words)\n",
    "                                            dataset_start = model_idx + 1\n",
    "                                            if dataset_start < len(parts):\n",
    "                                                if dataset_start + 1 < len(parts) and parts[dataset_start + 1] in ['Balanced', 'Original']:\n",
    "                                                    dataset_str = ' '.join(parts[dataset_start:dataset_start+2])\n",
    "                                                    num_start = dataset_start + 2\n",
    "                                                else:\n",
    "                                                    dataset_str = parts[dataset_start]\n",
    "                                                    num_start = dataset_start + 1\n",
    "                                                \n",
    "                                                dataset_type = 'raw' if 'Raw' in dataset_str or 'Original' in dataset_str else ('smote' if 'SMOTE' in dataset_str else 'adasyn')\n",
    "                                                \n",
    "                                                # Extract numeric values\n",
    "                                                nums = []\n",
    "                                                for p in parts[num_start:]:\n",
    "                                                    try:\n",
    "                                                        nums.append(float(p))\n",
    "                                                    except:\n",
    "                                                        pass\n",
    "                                                \n",
    "                                                if len(nums) >= 5 and model:\n",
    "                                                    metrics_list.append({\n",
    "                                                        'model_name': model,\n",
    "                                                        'dataset_type': dataset_type,\n",
    "                                                        'accuracy': nums[0],\n",
    "                                                        'precision_positive': nums[1],\n",
    "                                                        'recall_positive': nums[2],\n",
    "                                                        'f1_positive': nums[3],\n",
    "                                                        'roc_auc': nums[4],\n",
    "                                                        'tn': 0, 'fp': 0, 'fn': 0, 'tp': 0\n",
    "                                                    })\n",
    "                                        except Exception as e:\n",
    "                                            pass\n",
    "                        \n",
    "                        # Extract confusion matrix components\n",
    "                        if 'Confusion Matrix Components' in text:\n",
    "                            lines = text.split('\\n')\n",
    "                            for line in lines:\n",
    "                                if 'SVC' in line or 'MLP' in line:\n",
    "                                    # Parse line like: \"       SVC    Raw/Original 229  18  24  23\"\n",
    "                                    parts = line.split()\n",
    "                                    if len(parts) >= 7:\n",
    "                                        try:\n",
    "                                            # Find model name\n",
    "                                            model_idx = 0\n",
    "                                            while model_idx < len(parts) and parts[model_idx] not in ['SVC', 'MLP']:\n",
    "                                                model_idx += 1\n",
    "                                            model = parts[model_idx] if model_idx < len(parts) else None\n",
    "                                            \n",
    "                                            # Find dataset\n",
    "                                            dataset_start = model_idx + 1\n",
    "                                            if dataset_start < len(parts):\n",
    "                                                if dataset_start + 1 < len(parts) and parts[dataset_start + 1] in ['Balanced', 'Original']:\n",
    "                                                    dataset_str = ' '.join(parts[dataset_start:dataset_start+2])\n",
    "                                                    num_start = dataset_start + 2\n",
    "                                                else:\n",
    "                                                    dataset_str = parts[dataset_start]\n",
    "                                                    num_start = dataset_start + 1\n",
    "                                                \n",
    "                                                dataset_type = 'raw' if 'Raw' in dataset_str or 'Original' in dataset_str else ('smote' if 'SMOTE' in dataset_str else 'adasyn')\n",
    "                                                \n",
    "                                                # Get integers for confusion matrix\n",
    "                                                nums = [int(p) for p in parts[num_start:] if p.isdigit()]\n",
    "                                                if len(nums) >= 4 and model:\n",
    "                                                    key = f\"{model}_{dataset_type}\"\n",
    "                                                    cm_data[key] = {'tn': nums[0], 'fp': nums[1], 'fn': nums[2], 'tp': nums[3]}\n",
    "                                        except:\n",
    "                                            pass\n",
    "        \n",
    "        # Merge confusion matrix data into metrics\n",
    "        for metric in metrics_list:\n",
    "            key = f\"{metric['model_name']}_{metric['dataset_type']}\"\n",
    "            if key in cm_data:\n",
    "                metric.update(cm_data[key])\n",
    "        \n",
    "        return metrics_list\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting from notebook 04: {e}\")\n",
    "        return []\n",
    "\n",
    "# Extract metrics from all notebooks\n",
    "print(\"=\" * 70)\n",
    "print(\"EXTRACTING METRICS FROM ALL NOTEBOOKS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "all_combined_metrics = []\n",
    "\n",
    "# Extract from notebook 03\n",
    "print(\"\\nExtracting from notebook 03 (Tree-based models)...\")\n",
    "metrics_tree = extract_metrics_from_notebook03()\n",
    "print(f\"  Found {len(metrics_tree)} entries from notebook 03\")\n",
    "if len(metrics_tree) > 0:\n",
    "    all_combined_metrics.extend(metrics_tree)\n",
    "    print(f\"  Models found: {set(m['model_name'] for m in metrics_tree)}\")\n",
    "\n",
    "# Extract from notebook 04\n",
    "print(\"\\nExtracting from notebook 04 (SVC and MLP)...\")\n",
    "metrics_svc_mlp = extract_metrics_from_notebook04()\n",
    "print(f\"  Found {len(metrics_svc_mlp)} entries from notebook 04\")\n",
    "if len(metrics_svc_mlp) > 0:\n",
    "    all_combined_metrics.extend(metrics_svc_mlp)\n",
    "    print(f\"  Models found: {set(m['model_name'] for m in metrics_svc_mlp)}\")\n",
    "\n",
    "# Note: Notebook 02 (Logistic Regression) uses a different format\n",
    "# Manual entry section below\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"NOTEBOOK 02 (Logistic Regression) - Manual Entry Required\")\n",
    "print(\"=\" * 70)\n",
    "print(\"After running notebook 02, extract metrics from model_results['comparison_df']\")\n",
    "print(\"and model_results['raw/smote/adasyn']['confusion_matrix']\")\n",
    "print(\"\\nTemplate (uncomment and fill after running notebook 02):\")\n",
    "print(\"# metrics_logreg = [\")\n",
    "print(\"#     {'model_name': 'LogisticRegression', 'dataset_type': 'raw', ...},\")\n",
    "print(\"#     {'model_name': 'LogisticRegression', 'dataset_type': 'smote', ...},\")\n",
    "print(\"#     {'model_name': 'LogisticRegression', 'dataset_type': 'adasyn', ...}\")\n",
    "print(\"# ]\")\n",
    "print(\"# all_combined_metrics.extend(metrics_logreg)\")\n",
    "\n",
    "# Create DataFrame\n",
    "if len(all_combined_metrics) > 0:\n",
    "    metrics_df = pd.DataFrame(all_combined_metrics)\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"SUCCESS: Combined {len(metrics_df)} model-dataset combinations\")\n",
    "    print(f\"Models: {sorted(metrics_df['model_name'].unique().tolist())}\")\n",
    "    print(f\"Datasets: {sorted(metrics_df['dataset_type'].unique().tolist())}\")\n",
    "    print(f\"{'='*70}\")\n",
    "else:\n",
    "    metrics_df = pd.DataFrame(columns=[\n",
    "        'model_name', 'dataset_type',\n",
    "        'accuracy', 'precision_positive', 'recall_positive', 'f1_positive', 'roc_auc',\n",
    "        'tn', 'fp', 'fn', 'tp'\n",
    "    ])\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"WARNING: No metrics extracted.\")\n",
    "    print(\"Please ensure notebooks 03 and 04 have been executed completely.\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "metrics_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8efb508e",
   "metadata": {},
   "source": [
    "## Step 1b: Manual Entry for Notebook 02 (Logistic Regression)\n",
    "\n",
    "If automatic extraction didn't find notebook 02 metrics, manually add them here after running notebook 02.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "98917440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uncomment and fill in metrics_logreg above after running notebook 02\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Manual Entry for Notebook 02: Logistic Regression\n",
    "# ============================================================\n",
    "# After running notebook 02, copy the metrics here from model_results\n",
    "\n",
    "# Example format (uncomment and fill with actual values from notebook 02):\n",
    "# metrics_logreg = [\n",
    "#     {\n",
    "#         'model_name': 'LogisticRegression',\n",
    "#         'dataset_type': 'raw',\n",
    "#         'accuracy': 0.8934,  # From model_results['comparison_df'] for Raw/Original\n",
    "#         'precision_positive': 0.8158,\n",
    "#         'recall_positive': 0.4366,\n",
    "#         'f1_positive': 0.5688,\n",
    "#         'roc_auc': 0.8165,\n",
    "#         'tn': 0,  # Extract from model_results['raw']['confusion_matrix']\n",
    "#         'fp': 0,\n",
    "#         'fn': 0,\n",
    "#         'tp': 0\n",
    "#     },\n",
    "#     {\n",
    "#         'model_name': 'LogisticRegression',\n",
    "#         'dataset_type': 'smote',\n",
    "#         'accuracy': 0.8073,\n",
    "#         'precision_positive': 0.4286,\n",
    "#         'recall_positive': 0.5915,\n",
    "#         'f1_positive': 0.4970,\n",
    "#         'roc_auc': 0.7977,\n",
    "#         'tn': 0,\n",
    "#         'fp': 0,\n",
    "#         'fn': 0,\n",
    "#         'tp': 0\n",
    "#     },\n",
    "#     {\n",
    "#         'model_name': 'LogisticRegression',\n",
    "#         'dataset_type': 'adasyn',\n",
    "#         'accuracy': 0.8141,\n",
    "#         'precision_positive': 0.4421,\n",
    "#         'recall_positive': 0.5915,\n",
    "#         'f1_positive': 0.5060,\n",
    "#         'roc_auc': 0.8002,\n",
    "#         'tn': 0,\n",
    "#         'fp': 0,\n",
    "#         'fn': 0,\n",
    "#         'tp': 0\n",
    "#     }\n",
    "# ]\n",
    "\n",
    "# Uncomment to add to combined metrics:\n",
    "# if 'metrics_logreg' in locals():\n",
    "#     all_combined_metrics.extend(metrics_logreg)\n",
    "#     metrics_df = pd.DataFrame(all_combined_metrics)\n",
    "#     print(f\"Added {len(metrics_logreg)} Logistic Regression entries. Total: {len(metrics_df)}\")\n",
    "\n",
    "print(\"Uncomment and fill in metrics_logreg above after running notebook 02\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c1cb4c4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics DataFrame is empty. Please run notebooks 03 and 04, and optionally add notebook 02 metrics manually.\n"
     ]
    }
   ],
   "source": [
    "# --- Defensive dataset name mapping ---\n",
    "if len(metrics_df) > 0:\n",
    "    if 'dataset' not in metrics_df.columns and 'dataset_type' in metrics_df.columns:\n",
    "        dataset_name_map = {'raw': 'Raw/Original', 'smote': 'SMOTE Balanced', 'adasyn': 'ADASYN Balanced'}\n",
    "        metrics_df['dataset'] = metrics_df['dataset_type'].map(dataset_name_map)\n",
    "    \n",
    "    print(f\"\\nFinal metrics DataFrame shape: {metrics_df.shape}\")\n",
    "    print(f\"Columns: {list(metrics_df.columns)}\")\n",
    "else:\n",
    "    print(\"\\nMetrics DataFrame is empty. Please run notebooks 03 and 04, and optionally add notebook 02 metrics manually.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d314238",
   "metadata": {},
   "source": [
    "## Step 1c: Verify Metrics Extraction\n",
    "\n",
    "Check what metrics were successfully extracted and what might be missing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a310fc11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "NO METRICS FOUND\n",
      "======================================================================\n",
      "\n",
      "Please:\n",
      "1. Ensure notebooks 03 and 04 have been executed completely\n",
      "2. Re-run the extraction cell above\n",
      "3. Optionally add notebook 02 metrics manually\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Verify Metrics Extraction\n",
    "# ============================================================\n",
    "\n",
    "if len(metrics_df) > 0:\n",
    "    print(\"=\" * 70)\n",
    "    print(\"METRICS EXTRACTION SUMMARY\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    print(f\"\\nTotal model-dataset combinations: {len(metrics_df)}\")\n",
    "    print(f\"\\nModels found: {sorted(metrics_df['model_name'].unique().tolist())}\")\n",
    "    print(f\"Datasets found: {sorted(metrics_df['dataset_type'].unique().tolist())}\")\n",
    "    \n",
    "    # Check for missing models\n",
    "    expected_models = ['LogisticRegression', 'DecisionTree', 'RandomForest', 'GradientBoosting', 'SVC', 'MLP']\n",
    "    found_models = set(metrics_df['model_name'].unique())\n",
    "    missing_models = set(expected_models) - found_models\n",
    "    \n",
    "    if missing_models:\n",
    "        print(f\"\\n‚ö†Ô∏è  Missing models: {sorted(missing_models)}\")\n",
    "        print(\"   - Run the corresponding notebooks and re-run extraction\")\n",
    "    else:\n",
    "        print(\"\\n‚úì All expected models found!\")\n",
    "    \n",
    "    # Check for missing datasets\n",
    "    expected_datasets = ['raw', 'smote', 'adasyn']\n",
    "    found_datasets = set(metrics_df['dataset_type'].unique())\n",
    "    missing_datasets = set(expected_datasets) - found_datasets\n",
    "    \n",
    "    if missing_datasets:\n",
    "        print(f\"\\n‚ö†Ô∏è  Missing datasets: {sorted(missing_datasets)}\")\n",
    "    else:\n",
    "        print(\"\\n‚úì All expected datasets found!\")\n",
    "    \n",
    "    # Check for missing confusion matrix data\n",
    "    missing_cm = metrics_df[(metrics_df['tn'] == 0) & (metrics_df['fp'] == 0) & \n",
    "                            (metrics_df['fn'] == 0) & (metrics_df['tp'] == 0)]\n",
    "    if len(missing_cm) > 0:\n",
    "        print(f\"\\n‚ö†Ô∏è  {len(missing_cm)} entries missing confusion matrix data\")\n",
    "        print(\"   These will need to be filled manually or re-extracted\")\n",
    "    else:\n",
    "        print(\"\\n‚úì All confusion matrix data present!\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"Ready for comparison analysis!\")\n",
    "    print(\"=\" * 70)\n",
    "else:\n",
    "    print(\"=\" * 70)\n",
    "    print(\"NO METRICS FOUND\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"\\nPlease:\")\n",
    "    print(\"1. Ensure notebooks 03 and 04 have been executed completely\")\n",
    "    print(\"2. Re-run the extraction cell above\")\n",
    "    print(\"3. Optionally add notebook 02 metrics manually\")\n",
    "    print(\"=\" * 70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9259dd",
   "metadata": {},
   "source": [
    "## Step 2: Model Performance Comparison by Metric\n",
    "\n",
    "This section creates tables and visualizations for business and technical review:\n",
    "- One table for all model/dataset/metrics\n",
    "- Highlight the best models for each metric (recall, f1, ROC-AUC)\n",
    "- Plot bar charts for top-scoring models per metric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c0c3debb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paste or load your comparison DataFrame in the previous cell!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Summary Table: All Models, All Metrics\n",
    "# ============================================================\n",
    "\n",
    "if len(metrics_df) == 0:\n",
    "    print('Paste or load your comparison DataFrame in the previous cell!')\n",
    "else:\n",
    "    # Map dataset_type to readable names if not present\n",
    "    if 'dataset' not in metrics_df:\n",
    "        dataset_name_map = {'raw': 'Raw/Original', 'smote': 'SMOTE Balanced', 'adasyn': 'ADASYN Balanced'}\n",
    "        metrics_df['dataset'] = metrics_df['dataset_type'].map(dataset_name_map)\n",
    "\n",
    "    display_cols = [\n",
    "        'model_name', 'dataset', 'accuracy', 'precision_positive', 'recall_positive', 'f1_positive', 'roc_auc', 'tn', 'fp', 'fn', 'tp'\n",
    "    ]\n",
    "    df_display = metrics_df[display_cols].copy()\n",
    "    # Highlight top for each metric\n",
    "    highlight = {}\n",
    "    for m in ['recall_positive', 'f1_positive', 'roc_auc']:\n",
    "        idx = df_display[m].astype(float).idxmax()\n",
    "        if idx not in highlight:\n",
    "            highlight[idx] = m\n",
    "\n",
    "    def color_max(s):\n",
    "        color = ['background-color: #b3ffb3' if i in highlight and highlight[i]==s.name else '' for i in s.index]\n",
    "        return color\n",
    "\n",
    "    try:\n",
    "        display(df_display.style.apply(color_max))\n",
    "    except:\n",
    "        print(df_display)\n",
    "\n",
    "    print('\\nBest models by metric:')\n",
    "    for m in ['recall_positive', 'f1_positive', 'roc_auc']:\n",
    "        best_row = df_display.loc[df_display[m].astype(float).idxmax()]\n",
    "        print(f\"- {m}: {best_row['model_name']} on {best_row['dataset']} (score = {best_row[m]:.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3d15df29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No results to plot. Please load or paste your model metrics in the table above.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Bar Plots: Model Comparison by Recall, F1, ROC-AUC\n",
    "# ============================================================\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "if metrics_df.empty:\n",
    "    print('No results to plot. Please load or paste your model metrics in the table above.')\n",
    "else:\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    metrics_to_plot = [('recall_positive', 'Recall'), ('f1_positive', 'F1 Score'), ('roc_auc', 'ROC-AUC')]\n",
    "\n",
    "    for i, (col, title) in enumerate(metrics_to_plot):\n",
    "        top = metrics_df.groupby(['model_name', 'dataset'])[[col]].max().reset_index()\n",
    "        sns.barplot(data=top, x='model_name', y=col, hue='dataset', ax=axes[i])\n",
    "        axes[i].set_title(f'Models Comparison: {title}')\n",
    "        axes[i].set_ylim(0, 1)\n",
    "        axes[i].legend(title='Dataset')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc4e381",
   "metadata": {},
   "source": [
    "## Step 3: Confusion Matrix Heatmaps for Top Models\n",
    "\n",
    "Show confusion matrices for the models with the best recall, F1, and ROC-AUC (one per metric).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a714e637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No confusion matrices to visualize. Please load or paste your model metrics in the table above.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Confusion Matrix Heatmaps for Top Models (Recall, F1, ROC-AUC)\n",
    "# ============================================================\n",
    "\n",
    "if metrics_df.empty:\n",
    "    print('No confusion matrices to visualize. Please load or paste your model metrics in the table above.')\n",
    "else:\n",
    "    def plot_conf_matrix(row, metric_label):\n",
    "        cm = np.array([[row['tn'], row['fp']], [row['fn'], row['tp']]])\n",
    "        plt.figure(figsize=(4, 3))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                    xticklabels=['Pred No', 'Pred Yes'],\n",
    "                    yticklabels=['Actual No', 'Actual Yes'])\n",
    "        plt.title(f\"{row['model_name']} ‚Äì {row['dataset']}\\nConfusion Matrix [{metric_label}]\")\n",
    "        plt.ylabel('Actual')\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # Plot for models with max recall, f1, roc-auc\n",
    "    for metric_label in ['recall_positive', 'f1_positive', 'roc_auc']:\n",
    "        idx = metrics_df[metric_label].astype(float).idxmax()\n",
    "        row = metrics_df.loc[idx]\n",
    "        print(f\"\\nConfusion Matrix for best {metric_label} model:\")\n",
    "        plot_conf_matrix(row, metric_label)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac19a017",
   "metadata": {},
   "source": [
    "## Step 4: Business-Oriented Model Recommendation\n",
    "\n",
    "Report best models per metric and the corresponding confusion matrix breakdown for business action:\n",
    "- Which model catches the most at-risk employees (best recall)?\n",
    "- What is the trade-off between false alarms and missed cases among the winning models?\n",
    "\n",
    "üì¢ **Copy/paste or summarize these conclusions in any business summary deck.**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
